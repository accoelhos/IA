{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain \n",
    "\n",
    "Este noteboook ilustra os conceitos bÃ¡sicos da biblioteca LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76cb6bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install LangChain and required dependencies (uncomment if running in a new environment)\n",
    "# !pip install langchain openai duckduckgo-search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e5cc84",
   "metadata": {},
   "source": [
    "## Chat Models\n",
    "\n",
    "O conceito de **\"chat model\"** refere-se a modelos de linguagem que seguem o formato de **conversas estruturadas**.\n",
    "\n",
    "> Um **chat model** Ã© um modelo que recebe uma **sequÃªncia de mensagens** com papÃ©is explÃ­citos (`system`, `user`, `assistant`) e responde de forma contextualizada, como em um chat.\n",
    "\n",
    "Um chat model nÃ£o recebe apenas um `prompt`, mas sim uma **lista de mensagens** como esta:\n",
    "\n",
    "```python\n",
    "[\n",
    "  {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "  {\"role\": \"user\", \"content\": \"What is the capital of Brazil?\"}\n",
    "]\n",
    "```\n",
    "\n",
    "O modelo usa esse histÃ³rico para gerar uma resposta como:\n",
    "\n",
    "```json\n",
    "{\"role\": \"assistant\", \"content\": \"The capital of Brazil is BrasÃ­lia.\"}\n",
    "```\n",
    "\n",
    "Na LangChain, os chat models sÃ£o instÃ¢ncias de classes como:\n",
    "\n",
    "  * `ChatOpenAI`\n",
    "  * `ChatOllama`\n",
    "  * `ChatAnthropic`\n",
    "\n",
    "Eles seguem a interface da classe base `BaseChatModel`, e podem ser usados em cadeias (`Chains`), ferramentas (`Tools`) ou agentes (`Agents`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8092a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run get_llm.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0291c81f",
   "metadata": {},
   "source": [
    "## Tipos de mensagem\n",
    "\n",
    "Na LangChain, hÃ¡ trÃªs classes para representar mensagens em uma conversa. Essas classes estruturam a conversa para modelos de chat (como `ChatOpenAI` ou `ChatOllama`) e sÃ£o essenciais para manter **contexto, coerÃªncia e controle do comportamento** do agente.\n",
    "\n",
    "- `HumanMessage`: Representa uma **mensagem enviada pelo usuÃ¡rio**. Ã‰ o que o modelo deve interpretar como entrada.\n",
    "\n",
    "```python\n",
    "    HumanMessage(content=\"What is the weather like today?\")\n",
    "```\n",
    "\n",
    "- `AIMessage`: Representa uma **mensagem gerada pelo modelo (LLM)**. Ã‰ usada para manter o histÃ³rico da conversa.\n",
    "\n",
    "```python\n",
    "    AIMessage(content=\"The weather today is sunny and warm.\")\n",
    "```\n",
    "\n",
    "- `SystemMessage`: Fornece **instruÃ§Ãµes de configuraÃ§Ã£o ou contexto** para o modelo. Define o comportamento esperado.\n",
    "\n",
    "```python\n",
    "    SystemMessage(content=\"You are a helpful assistant that answers concisely.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f518ba6",
   "metadata": {},
   "source": [
    "## MÃ©todo predict_messages\n",
    "\n",
    "Um mÃ©todo (funÃ§Ã£o) importante da interface de `BaseChatModel` Ã© `predict_messages`. Essa funÃ§Ã£o recebe uma sequÃªncia de mensagens (user, system, assistant) e retorna a prÃ³xima mensagem do modelo, mantendo o formato conversacional.\n",
    "\n",
    "A sequÃªncia correta de mensagens na lista para chamar `predict_messages` deve sempre finalizar com `HumanMessage`, conforme a seguir:\n",
    "\n",
    "```\n",
    "[\n",
    "    SystemMessage(...),       # configura o comportamento\n",
    "    HumanMessage(...),        # pergunta 1\n",
    "    AIMessage(...),           # resposta 1\n",
    "    HumanMessage(...),        # pergunta 2 â† o modelo vai responder a essa\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25acf064",
   "metadata": {},
   "source": [
    "#### Exemplo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d304abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the AI break up with the robot? \n",
      "\n",
      "... Because it said, \"I need some space... and a little less processing power!\" ðŸ˜„ \n",
      "\n",
      "---\n",
      "\n",
      "Would you like to hear another joke?\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "\n",
    "llm = get_llm(model_backend=\"ollama\")\n",
    "response = llm.predict_messages([\n",
    "    HumanMessage(content=\"Tell me a joke about Artificial Intelligence.\")\n",
    "])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df0073b",
   "metadata": {},
   "source": [
    "#### Exemplo 2\n",
    "\n",
    "No exemplo abaixo, a funÃ§Ã£o `predict_messages` retorna um objeto `AIMessage` com o conteÃºdo gerado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07dfab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Brazil is **BrasÃ­lia**. \n",
      "\n",
      "Itâ€™s a fascinating city â€“ it was purpose-built as the capital in 1960! ðŸ˜Š \n",
      "\n",
      "Do you want to know anything more about BrasÃ­lia or Brazil in general?\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import SystemMessage\n",
    "\n",
    "response = llm.predict_messages([\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"What is the capital of Brazil?\")\n",
    "])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f08c7db",
   "metadata": {},
   "source": [
    "#### Exemplo 3\n",
    "\n",
    "Neste exemplo:\n",
    "\n",
    "- O modelo vÃª o contexto inteiro da conversa.\n",
    "\n",
    "- Ele sabe que jÃ¡ falou sobre Marie Curie antes.\n",
    "\n",
    "- A resposta Ã  segunda pergunta pode ser mais precisa e direta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0881573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marie Curie actually won **two** Nobel Prizes! Here's a breakdown of what she won them for:\n",
      "\n",
      "*   **1903 Nobel Prize in Physics:** This was jointly awarded to Marie, her husband Pierre Curie, and Henri Becquerel. They won for their research into **radioactivity**. Specifically, they investigated the properties of uranium rays, which Marie coined the term \"radioactivity\" to describe.\n",
      "\n",
      "*   **1911 Nobel Prize in Chemistry:** This prize was awarded solely to Marie Curie for the discovery of the elements **polonium** and **radium**, and for isolating radium. This was a huge accomplishment â€“ she was the first person to win Nobel Prizes in two different scientific fields.\n",
      "\n",
      "\n",
      "Itâ€™s important to note that her work was incredibly difficult and dangerous, as she worked with radioactive materials without fully understanding the risks.\n",
      "\n",
      "Do you want me to delve deeper into any specific aspect of her work or her life?\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "llm = get_llm(model_backend=\"ollama\")\n",
    "\n",
    "# HistÃ³rico da conversa\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that explains things clearly.\"),\n",
    "    \n",
    "    HumanMessage(content=\"Who is Marie Curie?\"),\n",
    "    AIMessage(content=\"Marie Curie was a pioneering physicist and chemist who conducted groundbreaking research on radioactivity. She was the first woman to win a Nobel Prize.\"),\n",
    "    \n",
    "    HumanMessage(content=\"What did she win the Nobel Prize for?\")\n",
    "]\n",
    "\n",
    "# O LLM irÃ¡ responder Ã  Ãºltima pergunta considerando o histÃ³rico\n",
    "response = llm.predict_messages(messages)\n",
    "\n",
    "# Imprime a resposta gerada\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c5f30",
   "metadata": {},
   "source": [
    "DiferenÃ§as entre chat model e LLM bÃ¡sico:\n",
    "\n",
    "| Aspecto           | Chat Model                             | LLM BÃ¡sico |\n",
    "| ----------------- | -------------------------------------- | ----------------------- |\n",
    "| Entrada           | Lista de mensagens (`role`, `content`) | Texto simples           |\n",
    "| Formato de prompt | Estruturado                            | Prompt plano            |\n",
    "| Exemplo tÃ­pico    | ChatGPT, Claude, Gemini, Gemma         | Text-Davinci-003, GPT-J |\n",
    "| Classe base       | `BaseChatModel`                        | `BaseLLM`               |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51863a17",
   "metadata": {},
   "source": [
    "## Cadeias (Chains)\n",
    "\n",
    "Em muitos casos simples, um chat model por si sÃ³ Ã© suficiente. Mas o conceito de `Chain` na LangChain vai alÃ©m de apenas manter histÃ³rico: ele organiza a execuÃ§Ã£o de etapas com reuso, modularidade e integraÃ§Ã£o de mÃºltiplos componentes.\n",
    "\n",
    "Por que usar apenas um chat model pode ser suficiente?\n",
    "\n",
    "Simplesmente isso:\n",
    "\n",
    "```python\n",
    "llm = get_llm()\n",
    "messages = [...]\n",
    "response = llm.predict_messages(messages)\n",
    "```\n",
    "\n",
    "resolve muitos casos â€” principalmente quando:\n",
    "\n",
    "* VocÃª **manualmente controla o histÃ³rico**\n",
    "* NÃ£o hÃ¡ necessidade de **prÃ©-processamento**, **pÃ³s-processamento**, ou **acesso a ferramentas**\n",
    "\n",
    "---\n",
    "\n",
    "EntÃ£o, por que usar uma `Chain`? Porque em aplicaÃ§Ãµes reais, geralmente precisamos estruturar algo como:\n",
    "\n",
    "> Entrada â†’ preparar prompt â†’ gerar resposta â†’ extrair algo â†’ formatar saÃ­da â†’ logar/armazenar\n",
    "\n",
    "E isso pode envolver:\n",
    "\n",
    "* IntegraÃ§Ã£o com **retrievers**\n",
    "* AplicaÃ§Ã£o de **memÃ³ria automÃ¡tica**\n",
    "* Uso de **ferramentas externas**\n",
    "* ExtraÃ§Ã£o de **informaÃ§Ãµes estruturadas**\n",
    "* PÃ³s-processamento da saÃ­da do LLM\n",
    "\n",
    "Comparando:\n",
    "\n",
    "| SituaÃ§Ã£o                                   | Chat Model direto | Chain recomendada? |\n",
    "| ------------------------------------------ | ----------------- | ------------------ |\n",
    "| Simples conversa em linguagem natural      | âœ… Sim             | âŒ NÃ£o precisa      |\n",
    "| Montagem dinÃ¢mica de prompt                | âš ï¸ Manual         | âœ… Sim              |\n",
    "| Consulta a base de conhecimento (RAG)      | âŒ NÃ£o             | âœ… Sim              |\n",
    "| MÃºltiplos passos (ex: busca + sumarizaÃ§Ã£o) | âŒ NÃ£o             | âœ… Sim              |\n",
    "| Logging, callbacks, parsing estruturado    | âŒ NÃ£o             | âœ… Sim              |\n",
    "\n",
    "Analogia didÃ¡tica:\n",
    "\n",
    "> Usar um chat model direto Ã© como usar uma **calculadora cientÃ­fica**: Ãºtil e pontual.\n",
    "> Usar uma `Chain` Ã© como **programar uma planilha com vÃ¡rias etapas**: vocÃª automatiza o processo, pode reutilizar partes, e conectar com outras fontes de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca2e963",
   "metadata": {},
   "source": [
    "#### Exemplo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b6c33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common and natural translation of \"I love apples\" in French is:\n",
      "\n",
      "**J'aime les pommes.**\n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "*   **J'aime** - I love\n",
      "*   **les** - the (plural)\n",
      "*   **pommes** - apples\n",
      "\n",
      "You could also say:\n",
      "\n",
      "*   **Je suis amoureux/amoureuse des pommes.** (I am in love with apples - more emphatic) - *amoureux* is for a male speaker, *amoureuse* for a female.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Translate to French: {text}\")\n",
    "llm = get_llm(model_backend=\"ollama\")\n",
    "\n",
    "# ComposiÃ§Ã£o de operadores\n",
    "chain = prompt | llm\n",
    "\n",
    "output = chain.invoke({\"text\": \"I love apples.\"})\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feee070",
   "metadata": {},
   "source": [
    "#### Exemplo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26132bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 1. Cadeia de resumo\n",
    "summary_prompt = PromptTemplate.from_template(\"Summarize the following text:\\n\\n{text}\")\n",
    "summary_chain = summary_prompt | get_llm(model_backend=\"ollama\")\n",
    "\n",
    "# 2. Cadeia de traduÃ§Ã£o\n",
    "translate_prompt = PromptTemplate.from_template(\"Translate to Portuguese:\\n\\n{text}\")\n",
    "translate_chain = translate_prompt | get_llm(model_backend=\"ollama\")\n",
    "\n",
    "# 3. Cadeia composta\n",
    "def full_chain(article_text):\n",
    "    summary = summary_chain.invoke({\"text\": article_text})\n",
    "    translated = translate_chain.invoke({\"text\": summary})\n",
    "    return {\"summary\": summary, \"translated\": translated}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3975df",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcac71ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load get_llm.py\n",
    "def get_llm(model_backend, model_name=None):\n",
    "    if model_backend == \"openai\":\n",
    "        from langchain.chat_models import ChatOpenAI\n",
    "        return ChatOpenAI(\n",
    "            temperature=0,\n",
    "            model=model_name or \"gpt-3.5-turbo\"\n",
    "        )\n",
    "    elif model_backend == \"ollama\":\n",
    "        from langchain_ollama import ChatOllama\n",
    "        return ChatOllama(\n",
    "            temperature=0,\n",
    "            model=model_name or \"gemma3:latest\"\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown backend: {model_backend}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdbfae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose between \"openai\" or \"ollama\"\n",
    "# MODEL_BACKEND = \"openai\"\n",
    "MODEL_BACKEND = \"ollama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0c3dfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "from langchain.chains.llm_math.base import LLMMathChain\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "\n",
    "# Initialize the language model\n",
    "llm = get_llm(MODEL_BACKEND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64d38b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tools the agent can use\n",
    "search = DuckDuckGoSearchRun()\n",
    "calculator = LLMMathChain.from_llm(llm)\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=search.run,\n",
    "        description=\"Useful for answering questions about current events or general knowledge.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Calculator\",\n",
    "        func=calculator.run,\n",
    "        description=\"Useful for solving mathematical problems.\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded9b1ad",
   "metadata": {},
   "source": [
    "Podemos inspecionar o prompt definido internamente na classe LLMMAthChain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bf38431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
      "\n",
      "Question: ${{Question with math problem.}}\n",
      "```text\n",
      "${{single line mathematical expression that solves the problem}}\n",
      "```\n",
      "...numexpr.evaluate(text)...\n",
      "```output\n",
      "${{Output of running the code}}\n",
      "```\n",
      "Answer: ${{Answer}}\n",
      "\n",
      "Begin.\n",
      "\n",
      "Question: What is 37593 * 67?\n",
      "```text\n",
      "37593 * 67\n",
      "```\n",
      "...numexpr.evaluate(\"37593 * 67\")...\n",
      "```output\n",
      "2518731\n",
      "```\n",
      "Answer: 2518731\n",
      "\n",
      "Question: 37593^(1/5)\n",
      "```text\n",
      "37593**(1/5)\n",
      "```\n",
      "...numexpr.evaluate(\"37593**(1/5)\")...\n",
      "```output\n",
      "8.222831614237718\n",
      "```\n",
      "Answer: 8.222831614237718\n",
      "\n",
      "Question: {question}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(calculator.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277b4018",
   "metadata": {},
   "source": [
    ">     agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION\n",
    "\n",
    "| Parte         | Significado                                                                                                                 |\n",
    "| ------------- | --------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `CHAT`        | Usa um modelo de chat (ex: `ChatOpenAI`, `ChatAnthropic`) que aceita mensagens estruturadas (`System`, `User`, `Assistant`) |\n",
    "| `ZERO_SHOT`   | NÃ£o requer exemplos (few-shot) para instruir o modelo â€” apenas uma descriÃ§Ã£o textual da tarefa e das ferramentas            |\n",
    "| `REACT`       | O agente segue o ciclo: **Thought â†’ Action â†’ Observation**                                                                  |\n",
    "| `DESCRIPTION` | As ferramentas sÃ£o descritas por texto, e o modelo escolhe qual usar com base nessas descriÃ§Ãµes                             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1ba46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ReAct agent\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9864e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to calculate the sum of 144 and 25, and then find the square root of that sum.\n",
      "Action:\n",
      "```json\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"144+25\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 169\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mNow I need to find the square root of 169.\n",
      "Action:\n",
      "```json\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"sqrt(169)\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 13.0\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: 13\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'13'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run an example query\n",
    "agent.run(\"What is the square root of the sum of 144 and 25?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef4d781",
   "metadata": {},
   "source": [
    "CHAT_ZERO_SHOT_REACT_DESCRIPTION x STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a6bfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to calculate the square root of 81. This is a straightforward calculation.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"sqrt(81)\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 9.0\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: 9\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'9'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import Tool, initialize_agent\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.chains.llm_math.base import LLMMathChain\n",
    "\n",
    "math_tool = Tool(\n",
    "    name=\"Calculator\",\n",
    "    func=LLMMathChain.from_llm(llm).run,\n",
    "    description=\"Useful for solving math problems.\"\n",
    ")\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=[math_tool],\n",
    "    llm=llm,\n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent.run(\"What is the square root of 81?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b103e2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: The question asks for the square root of 81. I can use the calculator tool to perform this calculation.\n",
      "Action:\n",
      "```json\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": {\n",
      "    \"expression\": \"sqrt(81)\"\n",
      "  }\n",
      "}\n",
      "```\u001b[0m"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sqrt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m      9\u001b[39m structured_math_tool = StructuredTool.from_function(\n\u001b[32m     10\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33mCalculator\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m     description=\u001b[33m\"\u001b[39m\u001b[33mEvaluate a mathematical expression\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m     func=sqrt_tool\n\u001b[32m     13\u001b[39m )\n\u001b[32m     15\u001b[39m agent = initialize_agent(\n\u001b[32m     16\u001b[39m     tools=[structured_math_tool],\n\u001b[32m     17\u001b[39m     llm=llm,\n\u001b[32m     18\u001b[39m     agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, \n\u001b[32m     19\u001b[39m     verbose=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     20\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is the square root of 81?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gcc1734/lib/python3.13/site-packages/langchain_core/_api/deprecation.py:189\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    188\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gcc1734/lib/python3.13/site-packages/langchain/chains/base.py:603\u001b[39m, in \u001b[36mChain.run\u001b[39m\u001b[34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[39m\n\u001b[32m    601\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) != \u001b[32m1\u001b[39m:\n\u001b[32m    602\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m`run` supports only one positional argument.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m603\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[32m    604\u001b[39m         _output_key\n\u001b[32m    605\u001b[39m     ]\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[32m    608\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[32m    609\u001b[39m         _output_key\n\u001b[32m    610\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gcc1734/lib/python3.13/site-packages/langchain_core/_api/deprecation.py:189\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    188\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gcc1734/lib/python3.13/site-packages/langchain/chains/base.py:386\u001b[39m, in \u001b[36mChain.__call__\u001b[39m\u001b[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[32m    355\u001b[39m \n\u001b[32m    356\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    377\u001b[39m \u001b[33;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[32m    378\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    379\u001b[39m config = {\n\u001b[32m    380\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks,\n\u001b[32m    381\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m: tags,\n\u001b[32m    382\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    383\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m: run_name,\n\u001b[32m    384\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gcc1734/lib/python3.13/site-packages/langchain/chains/base.py:167\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    166\u001b[39m     run_manager.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    168\u001b[39m run_manager.on_chain_end(outputs)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gcc1734/lib/python3.13/site-packages/langchain/chains/base.py:157\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    155\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    156\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    159\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    160\u001b[39m     )\n\u001b[32m    162\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    163\u001b[39m         inputs, outputs, return_only_outputs\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gcc1734/lib/python3.13/site-packages/langchain/agents/agent.py:1620\u001b[39m, in \u001b[36mAgentExecutor._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m   1618\u001b[39m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[32m   1619\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_continue(iterations, time_elapsed):\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m     next_step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1621\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1624\u001b[39m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1626\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1627\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[32m   1628\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._return(\n\u001b[32m   1629\u001b[39m             next_step_output, intermediate_steps, run_manager=run_manager\n\u001b[32m   1630\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gcc1734/lib/python3.13/site-packages/langchain/agents/agent.py:1328\u001b[39m, in \u001b[36mAgentExecutor._take_next_step\u001b[39m\u001b[34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[39m\n\u001b[32m   1317\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_take_next_step\u001b[39m(\n\u001b[32m   1318\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1319\u001b[39m     name_to_tool_map: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1323\u001b[39m     run_manager: Optional[CallbackManagerForChainRun] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1324\u001b[39m ) -> Union[AgentFinish, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[32m   1325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._consume_next_step(\n\u001b[32m   1326\u001b[39m         \u001b[43m[\u001b[49m\n\u001b[32m   1327\u001b[39m \u001b[43m            \u001b[49m\u001b[43ma\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1328\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1336\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gcc1734/lib/python3.13/site-packages/langchain/agents/agent.py:1411\u001b[39m, in \u001b[36mAgentExecutor._iter_next_step\u001b[39m\u001b[34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[39m\n\u001b[32m   1409\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m agent_action\n\u001b[32m   1410\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m agent_action \u001b[38;5;129;01min\u001b[39;00m actions:\n\u001b[32m-> \u001b[39m\u001b[32m1411\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_perform_agent_action\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\n\u001b[32m   1413\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gcc1734/lib/python3.13/site-packages/langchain/agents/agent.py:1433\u001b[39m, in \u001b[36mAgentExecutor._perform_agent_action\u001b[39m\u001b[34m(self, name_to_tool_map, color_mapping, agent_action, run_manager)\u001b[39m\n\u001b[32m   1431\u001b[39m         tool_run_kwargs[\u001b[33m\"\u001b[39m\u001b[33mllm_prefix\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1432\u001b[39m     \u001b[38;5;66;03m# We then call the tool on the tool input to get an observation\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1433\u001b[39m     observation = \u001b[43mtool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1434\u001b[39m \u001b[43m        \u001b[49m\u001b[43magent_action\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1435\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1436\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1437\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1438\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtool_run_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1439\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1440\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1441\u001b[39m     tool_run_kwargs = \u001b[38;5;28mself\u001b[39m._action_agent.tool_run_logging_kwargs()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gcc1734/lib/python3.13/site-packages/langchain_core/tools/base.py:883\u001b[39m, in \u001b[36mBaseTool.run\u001b[39m\u001b[34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[39m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_to_raise:\n\u001b[32m    882\u001b[39m     run_manager.on_tool_error(error_to_raise)\n\u001b[32m--> \u001b[39m\u001b[32m883\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error_to_raise\n\u001b[32m    884\u001b[39m output = _format_output(content, artifact, tool_call_id, \u001b[38;5;28mself\u001b[39m.name, status)\n\u001b[32m    885\u001b[39m run_manager.on_tool_end(output, color=color, name=\u001b[38;5;28mself\u001b[39m.name, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gcc1734/lib/python3.13/site-packages/langchain_core/tools/base.py:852\u001b[39m, in \u001b[36mBaseTool.run\u001b[39m\u001b[34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[39m\n\u001b[32m    850\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config_param := _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m._run):\n\u001b[32m    851\u001b[39m         tool_kwargs = tool_kwargs | {config_param: config}\n\u001b[32m--> \u001b[39m\u001b[32m852\u001b[39m     response = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mtool_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtool_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.response_format == \u001b[33m\"\u001b[39m\u001b[33mcontent_and_artifact\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    854\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(response) != \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/gcc1734/lib/python3.13/site-packages/langchain_core/tools/structured.py:93\u001b[39m, in \u001b[36mStructuredTool._run\u001b[39m\u001b[34m(self, config, run_manager, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config_param := _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m.func):\n\u001b[32m     92\u001b[39m         kwargs[config_param] = config\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mStructuredTool does not support sync invocation.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36msqrt_tool\u001b[39m\u001b[34m(expression)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msqrt_tool\u001b[39m(expression: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexpression\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:1\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'sqrt' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import Tool, initialize_agent\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.tools import StructuredTool\n",
    "\n",
    "def sqrt_tool(expression: str) -> str:\n",
    "    return str(eval(expression))\n",
    "\n",
    "structured_math_tool = StructuredTool.from_function(\n",
    "    name=\"Calculator\",\n",
    "    description=\"Evaluate a mathematical expression\",\n",
    "    func=sqrt_tool\n",
    ")\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=[structured_math_tool],\n",
    "    llm=llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent.run(\"What is the square root of 81?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcc1734",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
