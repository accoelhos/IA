# Parte 3.2: Q-learning Tabular - Discretização de Estados

---

## 1. Enunciado da Tarefa

> O algoritmo Q-learning, em sua forma tabular, requer que tanto o conjunto de estados quanto o de ações sejam finitos e enumeráveis. No caso do ambiente MountainCar-v0, o espaço de ações já é discreto. Contudo, o espaço de estados é contínuo (posição e velocidade).
>
> Assim, é necessário discretizar o espaço de estados... para que cada combinação possa ser representada como uma entrada discreta na tabela Q.
>
> 1. Divida a posição e velocidade em um número fixo de intervalos (bins);
> 2. Mapeie os estados contínuos para representações discretas;
> 3. Adapte o Q-learning tabular para treinar o agente neste espaço discretizado.

---

## 2. Metodologia e Explicação do Código

A Tabela Q é uma tabela que espera índices discretos (como 0, 1, 2...), mas o ambiente `MountainCar-v0` nos dá estados contínuos (como `[posição, velocidade]`).

A solução foi criar um "tradutor" (um wrapper de ambiente) que converte o estado contínuo em um ID de estado discreto.

Todo o trabalho foi feito no novo arquivo `mountaincar_environment.py`.

A nova classe herda de `Environment` e faz a discretização.

#### `__init__`

No construtor, definimos o tamanho da nossa grade de discretização. Foram escolhidas 20 "caixas" (bins) para a posição e 20 para a velocidade.

Usando `np.linspace` para criar 19 "divisórias" que definem nossas 20 caixas, usando os valores máximos e mínimos de posição e velocidade do próprio ambiente.

#### `get_num_states()`

A função agora retorna o número total de bins criados.
Se temos 10 bins de posição e 10 de velocidade, o número total de estados discretos é 20 * 20 = 400.

Isso permite que o agente (`QLearningAgentTabular`) crie uma Tabela Q com o tamanho correto (400 linhas).

#### `get_state_id(state)`

Esta é a função mais importante. É o "tradutor".

1.  Ela recebe o `state` contínuo do ambiente (ex: `[-0.45, 0.02]`).
2.  Usa `np.digitize` para descobrir em qual "caixa" (bin) cada valor (posição e velocidade) se encaixa.
3.  Combina esses dois índices de bin (ex: `pos_bin=8`, `vel_bin=5`) em um único ID de estado (ex: `85`).

A fórmula `pos_bin * self.num_vel_bins + vel_bin` garante que cada combinação de `(pos_bin, vel_bin)` tenha um ID único, de 0 a 99.

### Arquivo: `tql_train.py`

A única mudança aqui foi "registrar" o novo ambiente no dicionário:

### Arquivo: `tql.py` (O Agente)

**Uma modificação crucial foi necessária.**

Inicialmente, a tradução do estado (de `[-0.5, 0.01]` para um ID) estava sendo feita *fora* do agente, apenas no script `tql_train.py`.

Isso causou um `IndexError` no `tql_test.py` e no `tql_render_agent.py`. Esses scripts passavam o estado contínuo "cru" (o array) diretamente para o agente, e o agente tentava usar esse array como índice da Tabela Q, causando o erro.

**A Solução:**
A lógica de tradução foi movida para **dentro** do agente.
1.  O script `tql_train.py` foi simplificado e agora passa o estado "cru" para o agente.
2.  Os métodos `choose_action(self, state)` e `update(self, state, ...)` dentro do `tql.py` agora são responsáveis por chamar `self.env.get_state_id(state)` para obter o ID discreto antes de acessar a `self.q_table`.

Com isso, o agente sempre lida com a tradução internamente, e os scripts de treino, teste e renderização podem todos passar o estado "cru" (contínuo) para o agente, corrigindo o bug.

---

## 3. Comandos

* **Treinamento:**
```bash
python tql_train.py --env_name "MountainCar-v0" --num_episodes 7000 --decay_rate 0.0001 --learning_rate 0.1 --gamma 0.99
```

* **Teste:**
```bash
python tql_test.py --num_episodes 100 --env_name "MountainCar-v0"
```

* **Agente:**
```bash
python tql_render_agent.py --env_name "MountainCar-v0" --successful_episode_reward_value -199
```