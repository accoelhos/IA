# Parte 3.1: Q-learning Tabular - Replicação de Experimentos

---

## 1. Enunciado da Tarefa

> Utilize a classe `QLearningAgentTabular`, já fornecida. Repita os experimentos de treinamento e de teste dos agentes com os seguintes ambientes:
> * Blackjack
> * Cliff Walking
> * Frozen Lake
>
> Para cada ambiente, indique os hiperparâmetros usados no treinamento.

---

## 2. Metodologia: O que foi feito

1.  **Classe Base:** a classe `QLearningAgentTabular` (arquivo `tql.py`) funciona como o "cérebro" do agente. Ela já implementa a lógica principal do Q-learning, incluindo a regra de atualização e a estratégia de exploração (epsilon-greedy).

2.  **Wrappers de Ambiente:** O script de treinamento (`tql_train.py`) espera que cada ambiente do Gymnasium (`gym`) seja "envelopado" (wrapped) por uma classe customizada que herda da classe abstrata `Environment`. Essa classe wrapper serve como um adaptador, fornecendo uma interface consistente para o agente, especificamente os métodos:
    * `get_num_states()`: Retorna o número total de estados.
    * `get_num_actions()`: Retorna o número de ações possíveis.
    * `get_state_id(state)`: Converte a observação do ambiente (que pode ser um número, uma tupla, etc.) em um ID numérico único, que serve como índice para a Tabela Q.

3.  **Implementação dos Wrappers:**
    * O wrapper `BlackjackEnvironment` já estava parcialmente implementado e foi usado como base. Ele mapeia o estado `(soma_jogador, carta_dealer, ás_usável)` para um ID inteiro.
    * Para os ambientes `CliffWalking-v0` e `FrozenLake-v1`, foi necessário criar seus próprios wrappers: `CliffWalkingEnvironment` e `FrozenLakeEnvironment`.
    * Felizmente, nestes dois casos, o espaço de observação já é "Discreto" (Discrete), o que significa que o `state` retornado pelo ambiente já é o próprio ID numérico. Portanto, a implementação de `get_state_id(state)` foi simples: ela apenas retorna o próprio `state`.

4.  **Atualização do Treinador:** o dicionário `environment_dict` no arquivo `tql_train.py` foi atualizado para incluir as novas classes de ambiente, permitindo que o script as chame pela linha de comando.

---

## 3. Experimentos e Hiperparâmetros

Após a configuração, os agentes foram treinados para cada ambiente. A escolha dos hiperparâmetros foi feita através de experimentação para encontrar um equilíbrio entre tempo de treino e convergência da recompensa.

###  Ambiente 1: Blackjack-v1

* **Comando de Treinamento:**
```bash
 python tql_train.py --env_name "Blackjack-v1" --num_episodes 100000 --decay_rate 0.0001 --learning_rate 0.1 --gamma 0.99
```
* **Comando de Teste:**
```bash
python tql_test.py --num_episodes 1000 --env_name "Blackjack-v1"
```
* **Agente**:
```bash
python tql_render_agent.py --env_name "Blackjack-v1" --successful_episode_reward_value 1
```
 
* **Hiperparâmetros Usados:**
    * **Total de Episódios:** 100.000
    * **Taxa de Aprendizado (alpha):** 0.1
    * **Fator de Desconto (gamma):** 0.99
    * **Taxa de Decaimento (epsilon):** 0.0001

###  Ambiente 2: CliffWalking-v0

* **Comando de Treinamento:**
```bash
python tql_train.py --env_name "CliffWalking-v1" --num_episodes 10000 --decay_rate 0.001 --learning_rate 0.1 --gamma 0.99
```
* **Comando de Teste:**
```bash
python tql_test.py --num_episodes 100 --env_name "CliffWalking-v1"
```
* **Agente**:
```bash
python tql_render_agent.py --env_name "CliffWalking-v1" --successful_episode_reward_value -1
```
* **Hiperparâmetros Usados:**
    * **Total de Episódios:** 10.000
    * **Taxa de Aprendizado (alpha):** 0.1
    * **Fator de Desconto (gamma):** 0.99
    * **Taxa de Decaimento (epsilon):** 0.001

### Ambiente 3: FrozenLake-v1

* **Comando de Treinamento:**
```bash
python tql_train.py --env_name "FrozenLake-v1" --num_episodes 50000 --decay_rate 0.0001 --learning_rate 0.05 --gamma 0.95
```
* **Comando de Teste:**
```bash
python tql_test.py --num_episodes 1000 --env_name "FrozenLake-v1"
```
* **Agente**:
```bash
python tql_render_agent.py --env_name "FrozenLake-v1" --successful_episode_reward_value 1
```
* **Hiperparâmetros Usados:**
    * **Total de Episódios:** 50.000
    * **Taxa de Aprendizado (alpha):** 0.05
    * **Fator de Desconto (gamma):** 0.95
    * **Taxa de Decaimento (epsilon):** 0.0001