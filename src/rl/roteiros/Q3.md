# Parte 3.3: Q-learning com Aproximação Linear

---

## 1. Enunciado da Tarefa

A tarefa foi substituir a Tabela Q (Q-learning Tabular) por uma Aproximação de Função Linear.

Nesta abordagem, não armazenamos o valor Q de cada estado. Em vez disso, nós o *calculamos* como uma soma ponderada de "features" (características) do estado-ação:

$Q(s,a) = \sum_{i} w_i \cdot f_i(s,a)$

O agente (`QLearningAgentLinear`) aprende o vetor de pesos `w` usando o mesmo erro temporal (delta) de antes:

$\delta = r + \gamma \cdot \max_{a'} Q(s',a') - Q(s,a)$
$w_i \leftarrow w_i + \alpha \cdot \delta \cdot f_i(s,a)$

A tarefa foi definir os extratores de features $f_i(s,a)$ para os ambientes da 3.1 (Blackjack, Cliff, FrozenLake) e compará-los com a 3.2 (MountainCar).

---

## 2. Metodologia e Definição das Features

Modificação do `QLearningAgentLinear` (`lql.py`) para aceitar diferentes extratores de features dependendo do ambiente.

### A. Ambientes da 3.1 (Blackjack, CliffWalking, FrozenLake)

Para os ambientes discretos, foram usados extratores de features diferentes:

* **Blackjack-v1:**
    * Uso do `BlackjackFeatureExtractor` fornecido, que é complexo. Ele codifica manualmente features como "soma do jogador", "carta do dealer" e "jogador tem ás usável" em vetores binários.

* **CliffWalking-v1 e FrozenLake-v1:**
    * Para estes, foram criados novos extratores (`CliffWalkingFeatureExtractor`, `FrozenLakeFeatureExtractor`).
    * **Definição da Feature:** A "feature" é um **vetor One-Hot** que identifica unicamente o par (estado, ação).
    * Se o `CliffWalking` tem 48 estados e 4 ações, o número total de features (e o tamanho do vetor `w`) é $48 \times 4 = 192$.
    * A feature $f_i(s,a)$ é 1 se $i$ for o índice de $(s,a)$ e 0 caso contrário.
    * **Significado:** Esta abordagem é matematicamente equivalente ao Q-learning tabular. O peso `w_i` aprendido para a feature `i` (que representa `(s,a)`) é exatamente o valor `Q(s,a)` que teríamos na tabela. Isso serve para validar que o agente linear funciona.

### B. Comparação com 3.2 (MountainCar-v0)

Este é o caso mais importante, pois o ambiente é contínuo.

* **Problema:** Não podemos usar "one-hot", pois há infinitos estados.
* **Solução:** Usamos um extrator de features chamado **Tile Coding** (`tilecoder.py`), implementado no `MountainCarFeatureExtractor`.

#### O que é o Tile Coding (Codificador por Ladrilhos)?

1.  **Divisão em Bins (Grades):** Em vez de usar uma única grade (como na 3.2), o Tile Coder usa *múltiplas* grades (ex: 8 ou 10 grades).
2.  **Deslocamento (Offset):** Cada uma dessas grades é ligeiramente deslocada da outra. 
3.  **Generalização:** Quando o agente está em um estado contínuo `(posição, velocidade)`, esse estado ativa *um* "ladrilho" (tile) em *cada uma* das 8 grades.
4.  **Definição da Feature:** O vetor de features $f(s,a)$ é um vetor one-hot gigante, com um "1" para cada um dos 8 tiles ativados, na seção correspondente à ação `a`.
5.  **Significado:** Quando o agente aprende (atualiza os pesos `w`), ele atualiza o valor dos 8 tiles ativos. Se o agente se mover para um estado *próximo*, esse novo estado compartilhará *alguns* dos mesmos tiles. Graças a esse compartilhamento, o agente **generaliza** o que aprendeu em um estado para os estados vizinhos.

---

## 3. Comandos Executados

### Ambiente 1: Blackjack-v1

* **Comando de Treinamento:**
```bash
python lql_train.py --env_name "Blackjack-v1" --num_episodes 20000 --max_steps 100 --learning_rate 0.001 --epsilon_decay_rate 0.0001 --gamma 0.99
```
* **Comando de Teste:**
```bash
python lql_test.py --env_name "Blackjack-v1" --num_episodes 1000
```
* **Visualizar Agente (Render):**
```bash
python lql_render_agent.py --env_name "Blackjack-v1" --successful_episode_reward_value 1.0
```

### Ambiente 2: CliffWalking-v1

* **Comando de Treinamento:**
```bash
python lql_train.py --env_name "CliffWalking-v0" --num_episodes 5000 --max_steps 100 --learning_rate 0.1 --epsilon_decay_rate 0.001 --gamma 0.99
```
* **Comando de Teste:**
```bash
python lql_test.py --env_name "CliffWalking-v1" --num_episodes 100
```
* **Visualizar Agente (Render):**
```bash
python lql_render_agent.py --env_name "CliffWalking-v1" --successful_episode_reward_value -13
```

### Ambiente 3: FrozenLake-v1

* **Comando de Treinamento:**
```bash
python lql_train.py --env_name "FrozenLake-v1" --num_episodes 20000 --max_steps 100 --learning_rate 0.05 --epsilon_decay_rate 0.0001 --gamma 0.95
```
* **Comando de Teste:**
```bash
python lql_test.py --env_name "FrozenLake-v1" --num_episodes 1000
```
* **Visualizar Agente (Render):**
```bash
python lql_render_agent.py --env_name "FrozenLake-v1" --successful_episode_reward_value 1.0
```

### Ambiente 4: MountainCar-v0

*Nota: a `learning_rate` é $\alpha / N$, onde N é o número de tilings (8), por isso `0.0125`.*
* **Comando de Treinamento:**
```bash
python lql_train.py --env_name "MountainCar-v0" --num_episodes 5000 --max_steps 200 --learning_rate 0.0125 --epsilon_decay_rate 0.001 --gamma 1.0
```
* **Comando de Teste:**
```bash
python lql_test.py --env_name "MountainCar-v0" --num_episodes 100
```
* **Visualizar Agente (Render):**
```bash
python lql_render_agent.py --env_name "MountainCar-v0" --successful_episode_reward_value -199.0
```

---

## 4. Análise de Desempenho (Comparação)

A maior diferença é vista no `MountainCar-v0`.

* **3.2 Q-Learning Tabular (Grid Simples):**
    * **Problema:** Sem generalização. O agente precisa visitar todos os estados (100 ou 400) múltiplas vezes.
    * **Resultado:** Treinamento **extremamente lento** (ex: 50.000+ episódios) e dificuldade de convergência, como visto no experimento anterior.

* **3.3 Q-Learning Linear (Tile Coder):**
    * **Vantagem:** Generalização. O que é aprendido em um estado é aplicado aos estados vizinhos.
    * **Resultado:** Treinamento **drasticamente mais rápido**. O agente começa a resolver o ambiente consistentemente em menos de 2.000 a 5.000 episódios, uma melhoria de mais de 10x em eficiência de dados.

Os pesos `w` aprendidos no Tile Coder representam o "valor" de estar em uma determinada região (um tile) para uma determinada ação. Regiões que levam ao topo da colina (o objetivo) terão pesos positivos altos.