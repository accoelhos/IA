{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40fc61af",
   "metadata": {},
   "source": [
    "# Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590e5acc",
   "metadata": {},
   "source": [
    "\n",
    "> A **language model** is a computational system, typically implemented using machine learning (especially neural networks), that is trained to **predict and generate natural language text** by learning statistical patterns from large corpora — rather than being explicitly programmed with linguistic rules.\n",
    "\n",
    "> A machine learning-based **language model** is not hand-coded with grammar rules. Instead, it’s a type of machine learning model that learns how people use language by reading huge amounts of text. After training, it can complete sentences, answer questions, or write paragraphs — just by predicting what words are likely to come next.\n",
    "\n",
    "> A language model is like an **auto-complete on steroids** — trained on a huge amount of text, it can generate whole sentences, answer questions, write stories, or translate languages.\n",
    "\n",
    "### Think of it like this:\n",
    "\n",
    "* You start typing a message: “I am going to the…”\n",
    "* A language model can guess what comes next: “store”, “gym”, or “beach”.\n",
    "* It does this by learning patterns in how words are used together — like how we humans learn language by reading and listening."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febf5a9f",
   "metadata": {},
   "source": [
    "Here’s a simple example in Python using the `transformers` library from Hugging Face. This code shows how to use a **pre-trained language model** to generate text:\n",
    "\n",
    "### Example: Text Generation with GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6a89c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in a small village, a young woman came up to a young man and whispered to him, \"What is this man doing?\" The young man, who was about eight years old, said, \"I don't know.\" The young woman, who was about three years old, said, \"I don't know. He's not a boy.\" The old man, who was about eight years old, said, \"I don't know. I don't know. He's a girl.\" The young woman, who was about three years old, said, \"I don't know. I don't know. He's a boy.\" The young woman, who was about three years old, said, \"I don't know. I don't know. He's a girl.\" The old man, who was about three years old, said, \"I don't know. I don't know. He's a girl.\" The young woman, who was about three years old, said, \"I don't know. I don't know. He's a girl.\" The old man, who was about three years old, said, \"I don't know. I don't know. He's a girl.\" The young woman, who was about three years old, said, \"I don\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained language model (GPT-2)\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Give the model a prompt\n",
    "prompt = \"Once upon a time in a small village\"\n",
    "\n",
    "# Generate text\n",
    "result = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "\n",
    "# Print the result\n",
    "print(result[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff737d7",
   "metadata": {},
   "source": [
    "### What it does:\n",
    "\n",
    "* Loads **GPT-2**, a popular language model.\n",
    "* Takes your prompt `\"Once upon a time in a small village\"`.\n",
    "* Predicts and adds more text, word by word.\n",
    "\n",
    "You’ll get something like:\n",
    "\n",
    "```\n",
    "\"Once upon a time in a small village, there was a curious fox who loved to explore the nearby forest. Every morning...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a27322",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
