{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain - conceitos básicos\n",
    "\n",
    "Este noteboook ilustra os conceitos básicos da biblioteca LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e5cc84",
   "metadata": {},
   "source": [
    "## Modelos de conversação (*chat Models*)\n",
    "\n",
    "O conceito de **\"chat model\"** refere-se a modelos de linguagem que seguem o formato de **conversas estruturadas**.\n",
    "\n",
    "> Um **chat model** é um modelo que recebe uma **sequência de mensagens** com papéis explícitos (`system`, `user`, `assistant`) e responde de forma contextualizada, como em um chat.\n",
    "\n",
    "Um chat model recebe uma **lista de mensagens** como esta:\n",
    "\n",
    "```python\n",
    "  messages = [\n",
    "      SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "      HumanMessage(content=\"What is the capital of Brazil?\")\n",
    "  ]\n",
    "```\n",
    "\n",
    "O modelo usa esse histórico para gerar uma resposta como:\n",
    "\n",
    "```json\n",
    "  AIMessage(content=\"The capital of Brazil is Brasília.\")\n",
    "```\n",
    "\n",
    "Na LangChain, os chat models são instâncias de classes como:\n",
    "\n",
    "  * `ChatOpenAI`\n",
    "  * `ChatOllama`\n",
    "  * `ChatAnthropic`\n",
    "\n",
    "Eles seguem a interface da classe base `BaseChatModel`, e podem ser usados em cadeias (`Chains`), ferramentas (`Tools`) ou agentes (`Agents`).\n",
    "\n",
    "Abaixo está uma hierarquia simplificada da classe `BaseChatModel`, que é a superclasse abstrata dos modelos de linguagem com interface de chat no LangChain:\n",
    "\n",
    "```text\n",
    "BaseChatModel\n",
    "├── ChatOpenAI        → interface com os modelos da OpenAI (como gpt-3.5, gpt-4)\n",
    "├── ChatAnthropic     → interface com Claude (Anthropic)\n",
    "├── ChatOllama        → interface com modelos locais via Ollama (ex: gemma, llama3)\n",
    "├── ChatGooglePalm    → interface com o PaLM da Google\n",
    "├── ChatFireworks     → interface com modelos hospedados na FireworksAI\n",
    "├── ChatCohere        → interface com o Cohere Chat\n",
    "├── ChatAzureOpenAI   → variante da OpenAI para Azure\n",
    "├── ... (outras integrações via LangChainHub ou LangChain Community)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8092a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../get_llm.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0291c81f",
   "metadata": {},
   "source": [
    "## Mensagens\n",
    "\n",
    "Na LangChain, há várias classes para representar mensagens em uma conversa. Essas classes estruturam a conversa para modelos de chat (como `ChatOpenAI` ou `ChatOllama`) e são essenciais para manter **contexto, coerência e controle do comportamento** do agente. Os principais tipos de mensagens são os seguintes:\n",
    "\n",
    "- `HumanMessage`: Representa uma **mensagem enviada pelo usuário**. É o que o modelo deve interpretar como entrada.\n",
    "\n",
    "```python\n",
    "    HumanMessage(content=\"What is the weather like today?\")\n",
    "```\n",
    "\n",
    "- `AIMessage`: Representa uma **mensagem gerada pelo modelo (LLM)**. É usada para manter o histórico da conversa.\n",
    "\n",
    "```python\n",
    "    AIMessage(content=\"The weather today is sunny and warm.\")\n",
    "```\n",
    "\n",
    "- `SystemMessage`: Fornece **instruções de configuração ou contexto** para o modelo. Define o comportamento esperado.\n",
    "\n",
    "```python\n",
    "    SystemMessage(content=\"You are a helpful assistant that answers concisely.\")\n",
    "```\n",
    "\n",
    "No LangChain, a classe `ChatMessage` é a **superclasse** (ou classe base) de:\n",
    "\n",
    "* `HumanMessage`\n",
    "* `AIMessage`\n",
    "* `SystemMessage`\n",
    "* `FunctionMessage` (em alguns casos)\n",
    "* `ToolMessage` (LangChain mais recente)\n",
    "\n",
    "Hierarquia simplificada:\n",
    "\n",
    "```text\n",
    "ChatMessage\n",
    "├── HumanMessage     → representa uma mensagem escrita por um humano\n",
    "├── AIMessage        → representa a resposta do modelo (LLM)\n",
    "├── SystemMessage    → configura o comportamento ou tom do modelo\n",
    "├── FunctionMessage  → resultado da execução de uma função (caso use tools)\n",
    "├── ToolMessage      → resposta de uma ferramenta externa (LangChain Tools)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Propósito:\n",
    "\n",
    "* Permite que todas essas mensagens sejam tratadas de forma genérica como `ChatMessage`.\n",
    "* Você pode percorrer uma lista de mensagens (`List[ChatMessage]`) sem se preocupar com o tipo específico — útil para histórico de conversas.\n",
    "* Internamente, o modelo processa uma sequência de `ChatMessage`s com papéis diferentes.\n",
    "\n",
    "Exemplo:\n",
    "\n",
    "```python\n",
    "    from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "    mensagens = [\n",
    "        HumanMessage(content=\"Quem foi Marie Curie?\"),\n",
    "        AIMessage(content=\"Marie Curie foi uma cientista pioneira...\"),\n",
    "    ]\n",
    "\n",
    "    for m in mensagens:\n",
    "        print(f\"{type(m).__name__}: {m.content}\")\n",
    "```\n",
    "\n",
    "Saída:\n",
    "\n",
    "```\n",
    "HumanMessage: Quem foi Marie Curie?\n",
    "AIMessage: Marie Curie foi uma cientista pioneira...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eecee6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HumanMessage: Quem foi Marie Curie?\n",
      "AIMessage: Marie Curie foi uma cientista pioneira...\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "mensagens = [\n",
    "    HumanMessage(content=\"Quem foi Marie Curie?\"),\n",
    "    AIMessage(content=\"Marie Curie foi uma cientista pioneira...\"),\n",
    "]\n",
    "\n",
    "for m in mensagens:\n",
    "    print(f\"{type(m).__name__}: {m.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f518ba6",
   "metadata": {},
   "source": [
    "## Interação com o modelo\n",
    "\n",
    "Um método (função) importante da interface de `BaseChatModel` é `predict_messages`. Essa função é uma das diferentes formas (definidas na LangChain) para interagir com um LLM. Ela recebe uma sequência de mensagens (user, system, assistant) e retorna a próxima mensagem (gerada pelo modelo), mantendo o formato conversacional.\n",
    "\n",
    "A sequência correta de mensagens na lista para chamar `predict_messages` deve sempre finalizar com `HumanMessage`, conforme a seguir:\n",
    "\n",
    "```\n",
    "[\n",
    "    SystemMessage(...),       # configura o comportamento\n",
    "    HumanMessage(...),        # pergunta 1\n",
    "    AIMessage(...),           # resposta 1\n",
    "    HumanMessage(...),        # pergunta 2 ← o modelo vai responder a essa\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b93c944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../get_llm.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25acf064",
   "metadata": {},
   "source": [
    "#### Exemplo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d304abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_425047/875286630.py:4: LangChainDeprecationWarning: The method `BaseChatModel.predict_messages` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm.predict_messages([\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the AI break up with the robot? \n",
      "\n",
      "... Because it said, \"I need some space... and a little less processing power!\" 😄 \n",
      "\n",
      "---\n",
      "\n",
      "Would you like to hear another joke?\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "\n",
    "llm = get_llm(model_backend=\"ollama\")\n",
    "response = llm.predict_messages([\n",
    "    HumanMessage(content=\"Tell me a joke about Artificial Intelligence.\")\n",
    "])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df0073b",
   "metadata": {},
   "source": [
    "#### Exemplo 2\n",
    "\n",
    "No exemplo abaixo, a função `predict_messages` retorna um objeto `AIMessage` com o conteúdo gerado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c07dfab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Brazil is **Brasília**. \n",
      "\n",
      "It’s a fascinating city – it was purpose-built as the capital in 1960! 😊 \n",
      "\n",
      "Do you want to know anything more about Brasília or Brazil in general?\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import SystemMessage\n",
    "\n",
    "response = llm.predict_messages([\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"What is the capital of Brazil?\")\n",
    "])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f08c7db",
   "metadata": {},
   "source": [
    "#### Exemplo 3\n",
    "\n",
    "Neste exemplo:\n",
    "\n",
    "- O modelo vê o contexto inteiro da conversa.\n",
    "\n",
    "- Ele sabe que já falou sobre Marie Curie antes.\n",
    "\n",
    "- A resposta à segunda pergunta pode ser mais precisa e direta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0881573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marie Curie actually won **two** Nobel Prizes! Here's a breakdown of what she won them for:\n",
      "\n",
      "*   **1903 Nobel Prize in Physics:** This was jointly awarded to Marie, her husband Pierre Curie, and Henri Becquerel. They won for their research into **radioactivity**. Specifically, they investigated the properties of uranium rays, which Marie coined the term \"radioactivity\" to describe.\n",
      "\n",
      "*   **1911 Nobel Prize in Chemistry:** This prize was awarded solely to Marie Curie for the discovery of the elements **polonium** and **radium**, and for isolating radium. This was a huge accomplishment – she was the first person to win Nobel Prizes in two different scientific fields.\n",
      "\n",
      "\n",
      "It’s important to note that her work was incredibly difficult and dangerous, as she worked with radioactive materials without fully understanding the risks.\n",
      "\n",
      "Do you want me to delve deeper into any specific aspect of her work or her life?\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "llm = get_llm(model_backend=\"ollama\")\n",
    "\n",
    "# Histórico da conversa\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that explains things clearly.\"),\n",
    "    \n",
    "    HumanMessage(content=\"Who is Marie Curie?\"),\n",
    "    AIMessage(content=\"Marie Curie was a pioneering physicist and chemist who conducted groundbreaking research on radioactivity. She was the first woman to win a Nobel Prize.\"),\n",
    "    \n",
    "    HumanMessage(content=\"What did she win the Nobel Prize for?\")\n",
    "]\n",
    "\n",
    "# O LLM irá responder à última pergunta considerando o histórico\n",
    "response = llm.predict_messages(messages)\n",
    "\n",
    "# Imprime a resposta gerada\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c5f30",
   "metadata": {},
   "source": [
    "Diferenças entre chat model e LLM básico:\n",
    "\n",
    "| Aspecto           | Chat Model                             | LLM Básico |\n",
    "| ----------------- | -------------------------------------- | ----------------------- |\n",
    "| Entrada           | Lista de mensagens (`role`, `content`) | Texto simples           |\n",
    "| Formato de prompt | Estruturado                            | Prompt plano            |\n",
    "| Exemplo típico    | ChatGPT, Claude, Gemini, Gemma         | Text-Davinci-003, GPT-J |\n",
    "| Classe base       | `BaseChatModel`                        | `BaseLLM`               |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51863a17",
   "metadata": {},
   "source": [
    "## Cadeias (Chains)\n",
    "\n",
    "Em muitos casos simples, um chat model por si só é suficiente. Mas o conceito de `Chain` na LangChain vai além de apenas manter histórico: ele organiza a execução de etapas com reuso, modularidade e integração de múltiplos componentes.\n",
    "\n",
    "Por que usar apenas um chat model pode ser suficiente?\n",
    "\n",
    "Simplesmente isso:\n",
    "\n",
    "```python\n",
    "llm = get_llm()\n",
    "messages = [...]\n",
    "response = llm.predict_messages(messages)\n",
    "```\n",
    "\n",
    "resolve muitos casos — principalmente quando:\n",
    "\n",
    "* Você **manualmente controla o histórico**\n",
    "* Não há necessidade de **pré-processamento**, **pós-processamento**, ou **acesso a ferramentas**\n",
    "\n",
    "---\n",
    "\n",
    "Então, por que usar uma `Chain`? Porque em aplicações reais, geralmente precisamos estruturar algo como:\n",
    "\n",
    "> Entrada → preparar prompt → gerar resposta → extrair algo → formatar saída → logar/armazenar\n",
    "\n",
    "E isso pode envolver:\n",
    "\n",
    "* Integração com **retrievers**\n",
    "* Aplicação de **memória automática**\n",
    "* Uso de **ferramentas externas**\n",
    "* Extração de **informações estruturadas**\n",
    "* Pós-processamento da saída do LLM\n",
    "\n",
    "Comparando:\n",
    "\n",
    "| Situação                                   | Chat Model direto | Chain recomendada? |\n",
    "| ------------------------------------------ | ----------------- | ------------------ |\n",
    "| Simples conversa em linguagem natural      |  Sim             |  Não precisa      |\n",
    "| Montagem dinâmica de prompt                |  Manual         |  Sim              |\n",
    "| Consulta a base de conhecimento (RAG)      |  Não             |  Sim              |\n",
    "| Múltiplos passos (ex: busca + sumarização) |  Não             |  Sim              |\n",
    "| Logging, callbacks, parsing estruturado    |  Não             |  Sim              |\n",
    "\n",
    "Analogia didática:\n",
    "\n",
    "> Usar um chat model direto é como usar uma **calculadora científica**: útil e pontual.\n",
    "> Usar uma `Chain` é como **programar uma planilha com várias etapas**: você automatiza o processo, pode reutilizar partes, e conectar com outras fontes de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca2e963",
   "metadata": {},
   "source": [
    "#### Exemplo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58b6c33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common and natural translation of \"I love apples\" in French is:\n",
      "\n",
      "**J'aime les pommes.**\n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "*   **J'aime** - I love\n",
      "*   **les** - the (plural)\n",
      "*   **pommes** - apples\n",
      "\n",
      "You could also say:\n",
      "\n",
      "*   **J'adore les pommes.** (This is a stronger expression of love - \"I adore apples\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Translate to French: {text}\")\n",
    "llm = get_llm(model_backend=\"ollama\")\n",
    "\n",
    "# Composição de operadores\n",
    "chain = prompt | llm\n",
    "\n",
    "output = chain.invoke({\"text\": \"I love apples.\"})\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feee070",
   "metadata": {},
   "source": [
    "#### Exemplo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26132bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 1. Cadeia de resumo\n",
    "summary_prompt = PromptTemplate.from_template(\"Summarize the following text:\\n\\n{text}\")\n",
    "summary_chain = summary_prompt | get_llm(model_backend=\"ollama\")\n",
    "\n",
    "# 2. Cadeia de tradução\n",
    "translate_prompt = PromptTemplate.from_template(\"Translate to Portuguese:\\n\\n{text}\")\n",
    "translate_chain = translate_prompt | get_llm(model_backend=\"ollama\")\n",
    "\n",
    "# 3. Cadeia composta\n",
    "def full_chain(article_text):\n",
    "    summary = summary_chain.invoke({\"text\": article_text})\n",
    "    translated = translate_chain.invoke({\"text\": summary})\n",
    "    return {\"summary\": summary, \"translated\": translated}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3975df",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcac71ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load get_llm.py\n",
    "def get_llm(model_backend, model_name=None):\n",
    "    if model_backend == \"openai\":\n",
    "        from langchain.chat_models import ChatOpenAI\n",
    "        return ChatOpenAI(\n",
    "            temperature=0,\n",
    "            model=model_name or \"gpt-3.5-turbo\"\n",
    "        )\n",
    "    elif model_backend == \"ollama\":\n",
    "        from langchain_ollama import ChatOllama\n",
    "        return ChatOllama(\n",
    "            temperature=0,\n",
    "            model=model_name or \"gemma3:latest\"\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown backend: {model_backend}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdbfae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose between \"openai\" or \"ollama\"\n",
    "# MODEL_BACKEND = \"openai\"\n",
    "MODEL_BACKEND = \"ollama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0c3dfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "from langchain.chains.llm_math.base import LLMMathChain\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "\n",
    "# Initialize the language model\n",
    "llm = get_llm(MODEL_BACKEND)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded9b1ad",
   "metadata": {},
   "source": [
    "Podemos inspecionar o prompt definido internamente na classe LLMMAthChain:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277b4018",
   "metadata": {},
   "source": [
    ">     agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION\n",
    "\n",
    "| Parte         | Significado                                                                                                                 |\n",
    "| ------------- | --------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `CHAT`        | Usa um modelo de chat (ex: `ChatOpenAI`, `ChatAnthropic`) que aceita mensagens estruturadas (`System`, `User`, `Assistant`) |\n",
    "| `ZERO_SHOT`   | Não requer exemplos (few-shot) para instruir o modelo — apenas uma descrição textual da tarefa e das ferramentas            |\n",
    "| `REACT`       | O agente segue o ciclo: **Thought → Action → Observation**                                                                  |\n",
    "| `DESCRIPTION` | As ferramentas são descritas por texto, e o modelo escolhe qual usar com base nessas descrições                             |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
