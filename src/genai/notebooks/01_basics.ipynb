{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain - conceitos bÃ¡sicos\n",
    "\n",
    "Este noteboook ilustra os conceitos bÃ¡sicos da biblioteca LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e5cc84",
   "metadata": {},
   "source": [
    "## Modelos de conversaÃ§Ã£o (*chat Models*)\n",
    "\n",
    "O conceito de **\"chat model\"** refere-se a modelos de linguagem que seguem o formato de **conversas estruturadas**.\n",
    "\n",
    "> Um **chat model** Ã© um modelo que recebe uma **sequÃªncia de mensagens** com papÃ©is explÃ­citos (`system`, `user`, `assistant`) e responde de forma contextualizada, como em um chat.\n",
    "\n",
    "Um chat model recebe uma **lista de mensagens** como esta:\n",
    "\n",
    "```python\n",
    "  messages = [\n",
    "      SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "      HumanMessage(content=\"What is the capital of Brazil?\")\n",
    "  ]\n",
    "```\n",
    "\n",
    "O modelo usa esse histÃ³rico para gerar uma resposta como:\n",
    "\n",
    "```json\n",
    "  AIMessage(content=\"The capital of Brazil is BrasÃ­lia.\")\n",
    "```\n",
    "\n",
    "Na LangChain, os chat models sÃ£o instÃ¢ncias de classes como:\n",
    "\n",
    "  * `ChatOpenAI`\n",
    "  * `ChatOllama`\n",
    "  * `ChatAnthropic`\n",
    "\n",
    "Eles seguem a interface da classe base `BaseChatModel`, e podem ser usados em cadeias (`Chains`), ferramentas (`Tools`) ou agentes (`Agents`).\n",
    "\n",
    "Abaixo estÃ¡ uma hierarquia simplificada da classe `BaseChatModel`, que Ã© a superclasse abstrata dos modelos de linguagem com interface de chat no LangChain:\n",
    "\n",
    "```text\n",
    "BaseChatModel\n",
    "â”œâ”€â”€ ChatOpenAI        â†’ interface com os modelos da OpenAI (como gpt-3.5, gpt-4)\n",
    "â”œâ”€â”€ ChatAnthropic     â†’ interface com Claude (Anthropic)\n",
    "â”œâ”€â”€ ChatOllama        â†’ interface com modelos locais via Ollama (ex: gemma, llama3)\n",
    "â”œâ”€â”€ ChatGooglePalm    â†’ interface com o PaLM da Google\n",
    "â”œâ”€â”€ ChatFireworks     â†’ interface com modelos hospedados na FireworksAI\n",
    "â”œâ”€â”€ ChatCohere        â†’ interface com o Cohere Chat\n",
    "â”œâ”€â”€ ChatAzureOpenAI   â†’ variante da OpenAI para Azure\n",
    "â”œâ”€â”€ ... (outras integraÃ§Ãµes via LangChainHub ou LangChain Community)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8092a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../get_llm.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0291c81f",
   "metadata": {},
   "source": [
    "## Mensagens\n",
    "\n",
    "Na LangChain, hÃ¡ vÃ¡rias classes para representar mensagens em uma conversa. Essas classes estruturam a conversa para modelos de chat (como `ChatOpenAI` ou `ChatOllama`) e sÃ£o essenciais para manter **contexto, coerÃªncia e controle do comportamento** do agente. Os principais tipos de mensagens sÃ£o os seguintes:\n",
    "\n",
    "- `HumanMessage`: Representa uma **mensagem enviada pelo usuÃ¡rio**. Ã‰ o que o modelo deve interpretar como entrada.\n",
    "\n",
    "```python\n",
    "    HumanMessage(content=\"What is the weather like today?\")\n",
    "```\n",
    "\n",
    "- `AIMessage`: Representa uma **mensagem gerada pelo modelo (LLM)**. Ã‰ usada para manter o histÃ³rico da conversa.\n",
    "\n",
    "```python\n",
    "    AIMessage(content=\"The weather today is sunny and warm.\")\n",
    "```\n",
    "\n",
    "- `SystemMessage`: Fornece **instruÃ§Ãµes de configuraÃ§Ã£o ou contexto** para o modelo. Define o comportamento esperado.\n",
    "\n",
    "```python\n",
    "    SystemMessage(content=\"You are a helpful assistant that answers concisely.\")\n",
    "```\n",
    "\n",
    "No LangChain, a classe `ChatMessage` Ã© a **superclasse** (ou classe base) de:\n",
    "\n",
    "* `HumanMessage`\n",
    "* `AIMessage`\n",
    "* `SystemMessage`\n",
    "* `FunctionMessage` (em alguns casos)\n",
    "* `ToolMessage` (LangChain mais recente)\n",
    "\n",
    "Hierarquia simplificada:\n",
    "\n",
    "```text\n",
    "ChatMessage\n",
    "â”œâ”€â”€ HumanMessage     â†’ representa uma mensagem escrita por um humano\n",
    "â”œâ”€â”€ AIMessage        â†’ representa a resposta do modelo (LLM)\n",
    "â”œâ”€â”€ SystemMessage    â†’ configura o comportamento ou tom do modelo\n",
    "â”œâ”€â”€ FunctionMessage  â†’ resultado da execuÃ§Ã£o de uma funÃ§Ã£o (caso use tools)\n",
    "â”œâ”€â”€ ToolMessage      â†’ resposta de uma ferramenta externa (LangChain Tools)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "PropÃ³sito:\n",
    "\n",
    "* Permite que todas essas mensagens sejam tratadas de forma genÃ©rica como `ChatMessage`.\n",
    "* VocÃª pode percorrer uma lista de mensagens (`List[ChatMessage]`) sem se preocupar com o tipo especÃ­fico â€” Ãºtil para histÃ³rico de conversas.\n",
    "* Internamente, o modelo processa uma sequÃªncia de `ChatMessage`s com papÃ©is diferentes.\n",
    "\n",
    "Exemplo:\n",
    "\n",
    "```python\n",
    "    from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "    mensagens = [\n",
    "        HumanMessage(content=\"Quem foi Marie Curie?\"),\n",
    "        AIMessage(content=\"Marie Curie foi uma cientista pioneira...\"),\n",
    "    ]\n",
    "\n",
    "    for m in mensagens:\n",
    "        print(f\"{type(m).__name__}: {m.content}\")\n",
    "```\n",
    "\n",
    "SaÃ­da:\n",
    "\n",
    "```\n",
    "HumanMessage: Quem foi Marie Curie?\n",
    "AIMessage: Marie Curie foi uma cientista pioneira...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eecee6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HumanMessage: Quem foi Marie Curie?\n",
      "AIMessage: Marie Curie foi uma cientista pioneira...\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "mensagens = [\n",
    "    HumanMessage(content=\"Quem foi Marie Curie?\"),\n",
    "    AIMessage(content=\"Marie Curie foi uma cientista pioneira...\"),\n",
    "]\n",
    "\n",
    "for m in mensagens:\n",
    "    print(f\"{type(m).__name__}: {m.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f518ba6",
   "metadata": {},
   "source": [
    "## InteraÃ§Ã£o com o modelo\n",
    "\n",
    "Um mÃ©todo (funÃ§Ã£o) importante da interface de `BaseChatModel` Ã© `predict_messages`. Essa funÃ§Ã£o Ã© uma das diferentes formas (definidas na LangChain) para interagir com um LLM. Ela recebe uma sequÃªncia de mensagens (user, system, assistant) e retorna a prÃ³xima mensagem (gerada pelo modelo), mantendo o formato conversacional.\n",
    "\n",
    "A sequÃªncia correta de mensagens na lista para chamar `predict_messages` deve sempre finalizar com `HumanMessage`, conforme a seguir:\n",
    "\n",
    "```\n",
    "[\n",
    "    SystemMessage(...),       # configura o comportamento\n",
    "    HumanMessage(...),        # pergunta 1\n",
    "    AIMessage(...),           # resposta 1\n",
    "    HumanMessage(...),        # pergunta 2 â† o modelo vai responder a essa\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b93c944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../get_llm.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25acf064",
   "metadata": {},
   "source": [
    "#### Exemplo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d304abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_425047/875286630.py:4: LangChainDeprecationWarning: The method `BaseChatModel.predict_messages` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm.predict_messages([\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the AI break up with the robot? \n",
      "\n",
      "... Because it said, \"I need some space... and a little less processing power!\" ðŸ˜„ \n",
      "\n",
      "---\n",
      "\n",
      "Would you like to hear another joke?\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "\n",
    "llm = get_llm(model_backend=\"ollama\")\n",
    "response = llm.predict_messages([\n",
    "    HumanMessage(content=\"Tell me a joke about Artificial Intelligence.\")\n",
    "])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df0073b",
   "metadata": {},
   "source": [
    "#### Exemplo 2\n",
    "\n",
    "No exemplo abaixo, a funÃ§Ã£o `predict_messages` retorna um objeto `AIMessage` com o conteÃºdo gerado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c07dfab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Brazil is **BrasÃ­lia**. \n",
      "\n",
      "Itâ€™s a fascinating city â€“ it was purpose-built as the capital in 1960! ðŸ˜Š \n",
      "\n",
      "Do you want to know anything more about BrasÃ­lia or Brazil in general?\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import SystemMessage\n",
    "\n",
    "response = llm.predict_messages([\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"What is the capital of Brazil?\")\n",
    "])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f08c7db",
   "metadata": {},
   "source": [
    "#### Exemplo 3\n",
    "\n",
    "Neste exemplo:\n",
    "\n",
    "- O modelo vÃª o contexto inteiro da conversa.\n",
    "\n",
    "- Ele sabe que jÃ¡ falou sobre Marie Curie antes.\n",
    "\n",
    "- A resposta Ã  segunda pergunta pode ser mais precisa e direta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0881573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marie Curie actually won **two** Nobel Prizes! Here's a breakdown of what she won them for:\n",
      "\n",
      "*   **1903 Nobel Prize in Physics:** This was jointly awarded to Marie, her husband Pierre Curie, and Henri Becquerel. They won for their research into **radioactivity**. Specifically, they investigated the properties of uranium rays, which Marie coined the term \"radioactivity\" to describe.\n",
      "\n",
      "*   **1911 Nobel Prize in Chemistry:** This prize was awarded solely to Marie Curie for the discovery of the elements **polonium** and **radium**, and for isolating radium. This was a huge accomplishment â€“ she was the first person to win Nobel Prizes in two different scientific fields.\n",
      "\n",
      "\n",
      "Itâ€™s important to note that her work was incredibly difficult and dangerous, as she worked with radioactive materials without fully understanding the risks.\n",
      "\n",
      "Do you want me to delve deeper into any specific aspect of her work or her life?\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "llm = get_llm(model_backend=\"ollama\")\n",
    "\n",
    "# HistÃ³rico da conversa\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that explains things clearly.\"),\n",
    "    \n",
    "    HumanMessage(content=\"Who is Marie Curie?\"),\n",
    "    AIMessage(content=\"Marie Curie was a pioneering physicist and chemist who conducted groundbreaking research on radioactivity. She was the first woman to win a Nobel Prize.\"),\n",
    "    \n",
    "    HumanMessage(content=\"What did she win the Nobel Prize for?\")\n",
    "]\n",
    "\n",
    "# O LLM irÃ¡ responder Ã  Ãºltima pergunta considerando o histÃ³rico\n",
    "response = llm.predict_messages(messages)\n",
    "\n",
    "# Imprime a resposta gerada\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c5f30",
   "metadata": {},
   "source": [
    "DiferenÃ§as entre chat model e LLM bÃ¡sico:\n",
    "\n",
    "| Aspecto           | Chat Model                             | LLM BÃ¡sico |\n",
    "| ----------------- | -------------------------------------- | ----------------------- |\n",
    "| Entrada           | Lista de mensagens (`role`, `content`) | Texto simples           |\n",
    "| Formato de prompt | Estruturado                            | Prompt plano            |\n",
    "| Exemplo tÃ­pico    | ChatGPT, Claude, Gemini, Gemma         | Text-Davinci-003, GPT-J |\n",
    "| Classe base       | `BaseChatModel`                        | `BaseLLM`               |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51863a17",
   "metadata": {},
   "source": [
    "## Cadeias (Chains)\n",
    "\n",
    "Em muitos casos simples, um chat model por si sÃ³ Ã© suficiente. Mas o conceito de `Chain` na LangChain vai alÃ©m de apenas manter histÃ³rico: ele organiza a execuÃ§Ã£o de etapas com reuso, modularidade e integraÃ§Ã£o de mÃºltiplos componentes.\n",
    "\n",
    "Por que usar apenas um chat model pode ser suficiente?\n",
    "\n",
    "Simplesmente isso:\n",
    "\n",
    "```python\n",
    "llm = get_llm()\n",
    "messages = [...]\n",
    "response = llm.predict_messages(messages)\n",
    "```\n",
    "\n",
    "resolve muitos casos â€” principalmente quando:\n",
    "\n",
    "* VocÃª **manualmente controla o histÃ³rico**\n",
    "* NÃ£o hÃ¡ necessidade de **prÃ©-processamento**, **pÃ³s-processamento**, ou **acesso a ferramentas**\n",
    "\n",
    "---\n",
    "\n",
    "EntÃ£o, por que usar uma `Chain`? Porque em aplicaÃ§Ãµes reais, geralmente precisamos estruturar algo como:\n",
    "\n",
    "> Entrada â†’ preparar prompt â†’ gerar resposta â†’ extrair algo â†’ formatar saÃ­da â†’ logar/armazenar\n",
    "\n",
    "E isso pode envolver:\n",
    "\n",
    "* IntegraÃ§Ã£o com **retrievers**\n",
    "* AplicaÃ§Ã£o de **memÃ³ria automÃ¡tica**\n",
    "* Uso de **ferramentas externas**\n",
    "* ExtraÃ§Ã£o de **informaÃ§Ãµes estruturadas**\n",
    "* PÃ³s-processamento da saÃ­da do LLM\n",
    "\n",
    "Comparando:\n",
    "\n",
    "| SituaÃ§Ã£o                                   | Chat Model direto | Chain recomendada? |\n",
    "| ------------------------------------------ | ----------------- | ------------------ |\n",
    "| Simples conversa em linguagem natural      |  Sim             |  NÃ£o precisa      |\n",
    "| Montagem dinÃ¢mica de prompt                |  Manual         |  Sim              |\n",
    "| Consulta a base de conhecimento (RAG)      |  NÃ£o             |  Sim              |\n",
    "| MÃºltiplos passos (ex: busca + sumarizaÃ§Ã£o) |  NÃ£o             |  Sim              |\n",
    "| Logging, callbacks, parsing estruturado    |  NÃ£o             |  Sim              |\n",
    "\n",
    "Analogia didÃ¡tica:\n",
    "\n",
    "> Usar um chat model direto Ã© como usar uma **calculadora cientÃ­fica**: Ãºtil e pontual.\n",
    "> Usar uma `Chain` Ã© como **programar uma planilha com vÃ¡rias etapas**: vocÃª automatiza o processo, pode reutilizar partes, e conectar com outras fontes de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca2e963",
   "metadata": {},
   "source": [
    "#### Exemplo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58b6c33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common and natural translation of \"I love apples\" in French is:\n",
      "\n",
      "**J'aime les pommes.**\n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "*   **J'aime** - I love\n",
      "*   **les** - the (plural)\n",
      "*   **pommes** - apples\n",
      "\n",
      "You could also say:\n",
      "\n",
      "*   **J'adore les pommes.** (This is a stronger expression of love - \"I adore apples\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Translate to French: {text}\")\n",
    "llm = get_llm(model_backend=\"ollama\")\n",
    "\n",
    "# ComposiÃ§Ã£o de operadores\n",
    "chain = prompt | llm\n",
    "\n",
    "output = chain.invoke({\"text\": \"I love apples.\"})\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feee070",
   "metadata": {},
   "source": [
    "#### Exemplo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26132bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# 1. Cadeia de resumo\n",
    "summary_prompt = PromptTemplate.from_template(\"Summarize the following text:\\n\\n{text}\")\n",
    "summary_chain = summary_prompt | get_llm(model_backend=\"ollama\")\n",
    "\n",
    "# 2. Cadeia de traduÃ§Ã£o\n",
    "translate_prompt = PromptTemplate.from_template(\"Translate to Portuguese:\\n\\n{text}\")\n",
    "translate_chain = translate_prompt | get_llm(model_backend=\"ollama\")\n",
    "\n",
    "# 3. Cadeia composta\n",
    "def full_chain(article_text):\n",
    "    summary = summary_chain.invoke({\"text\": article_text})\n",
    "    translated = translate_chain.invoke({\"text\": summary})\n",
    "    return {\"summary\": summary, \"translated\": translated}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3975df",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcac71ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load get_llm.py\n",
    "def get_llm(model_backend, model_name=None):\n",
    "    if model_backend == \"openai\":\n",
    "        from langchain.chat_models import ChatOpenAI\n",
    "        return ChatOpenAI(\n",
    "            temperature=0,\n",
    "            model=model_name or \"gpt-3.5-turbo\"\n",
    "        )\n",
    "    elif model_backend == \"ollama\":\n",
    "        from langchain_ollama import ChatOllama\n",
    "        return ChatOllama(\n",
    "            temperature=0,\n",
    "            model=model_name or \"gemma3:latest\"\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown backend: {model_backend}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdbfae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose between \"openai\" or \"ollama\"\n",
    "# MODEL_BACKEND = \"openai\"\n",
    "MODEL_BACKEND = \"ollama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0c3dfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "from langchain.chains.llm_math.base import LLMMathChain\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "\n",
    "# Initialize the language model\n",
    "llm = get_llm(MODEL_BACKEND)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded9b1ad",
   "metadata": {},
   "source": [
    "Podemos inspecionar o prompt definido internamente na classe LLMMAthChain:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277b4018",
   "metadata": {},
   "source": [
    ">     agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION\n",
    "\n",
    "| Parte         | Significado                                                                                                                 |\n",
    "| ------------- | --------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `CHAT`        | Usa um modelo de chat (ex: `ChatOpenAI`, `ChatAnthropic`) que aceita mensagens estruturadas (`System`, `User`, `Assistant`) |\n",
    "| `ZERO_SHOT`   | NÃ£o requer exemplos (few-shot) para instruir o modelo â€” apenas uma descriÃ§Ã£o textual da tarefa e das ferramentas            |\n",
    "| `REACT`       | O agente segue o ciclo: **Thought â†’ Action â†’ Observation**                                                                  |\n",
    "| `DESCRIPTION` | As ferramentas sÃ£o descritas por texto, e o modelo escolhe qual usar com base nessas descriÃ§Ãµes                             |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
