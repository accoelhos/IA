{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64392c03",
   "metadata": {},
   "source": [
    "# Few-shot learning\n",
    "\n",
    "**Few-shot learning** é um paradigma de aprendizado de máquina no qual um modelo é treinado (ou ajustado) para realizar uma *tarefa de classificação* com **poucos exemplos rotulados por classe**. Tipicamente, entre 1 e 10 são usados. A ideia central é permitir que modelos generalizem bem, mesmo quando os dados rotulados são escassos, algo que humanos conseguem fazer com facilidade.\n",
    "\n",
    "Comparação com outros paradigmas:\n",
    "\n",
    "| Paradigma               | Nº de exemplos por classe | Exemplo típico                                       |\n",
    "| ----------------------- | ------------------------- | ---------------------------------------------------- |\n",
    "| **Supervised learning** | Centenas ou milhares      | Classificação de imagens com ImageNet                |\n",
    "| **Few-shot learning**   | De 1 a \\~10               | Identificar uma nova espécie de planta com 5 imagens |\n",
    "| **Zero-shot learning**  | 0 (nenhum exemplo)        | LLMs respondendo perguntas sem fine-tuning           |\n",
    "| **One-shot learning**   | 1 exemplo                 | Reconhecimento facial (ex: FaceNet)                  |\n",
    "\n",
    "Na prática, few-shot learning geralmente envolve um modelo **pré-treinado** e algum mecanismo de **adaptação com poucos dados**. Algumas abordagens comuns incluem:\n",
    "\n",
    "* **Meta-learning (aprendizado de como aprender):** o modelo é treinado em várias tarefas pequenas para aprender a se adaptar rapidamente a novas tarefas.\n",
    "\n",
    "  * Ex: MAML (Model-Agnostic Meta-Learning)\n",
    "* **Metric learning (aprendizado por distância):** aprende-se uma função de similaridade para comparar amostras (ex: prototypical networks, Siamese networks).\n",
    "* **Prompting em LLMs:** em vez de re-treinar o modelo, fornecemos exemplos diretamente no prompt (ex: “Given these 3 examples, what comes next?”).\n",
    "\n",
    "Como exemplo, imagine um LLM (e.g., Gemini, GPT-4, Llama) recebendo o seguinte prompt para classificar sentimentos:\n",
    "\n",
    "```text\n",
    "Exemplo 1: \"Estou muito feliz com o atendimento!\" → Sentimento: positivo  \n",
    "Exemplo 2: \"O produto chegou quebrado.\" → Sentimento: negativo  \n",
    "Exemplo 3: \"A entrega foi no prazo, mas o item não era o que esperava.\" → Sentimento: neutro  \n",
    "Texto: \"Gostei do produto, mas a entrega demorou.\" → Sentimento:\n",
    "```\n",
    "\n",
    "Esse é um exemplo de **few-shot prompting**, pois o modelo vê poucos exemplos antes de fazer a inferência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4705d85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../get_llm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b676b00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentimento classificado: Sentimento: neutro \n",
      "\n",
      "**Justificativa:** A frase contém um elemento positivo (\"o serviço foi bom\") e um elemento negativo (\"poderia ser mais rápido\"), mas o tom geral é de avaliação, e não de forte emoção positiva ou negativa.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Suponha que esta função já esteja definida em outro arquivo importado\n",
    "# def get_llm(model_backend=\"ollama\", model_name=None): ...\n",
    "\n",
    "# Exemplo de prompt few-shot\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"Classifique o sentimento das frases abaixo como positivo, negativo ou neutro.\n",
    "\n",
    "Exemplos:\n",
    "Frase: \"Estou muito feliz com o atendimento!\"  \n",
    "Sentimento: positivo\n",
    "\n",
    "Frase: \"O produto chegou quebrado e ninguém responde meu e-mail.\"  \n",
    "Sentimento: negativo\n",
    "\n",
    "Frase: \"A entrega foi no prazo, mas o item não era o que esperava.\"  \n",
    "Sentimento: neutro\n",
    "\n",
    "Agora, classifique a seguinte frase:\n",
    "Frase: \"{frase}\"  \n",
    "Sentimento:\"\"\"\n",
    ")\n",
    "\n",
    "# Constrói a cadeia com a nova API baseada em Runnable\n",
    "llm = get_llm()\n",
    "chain: Runnable = prompt_template | llm | StrOutputParser()\n",
    "\n",
    "# Exemplo de uso\n",
    "frase_teste = \"O serviço foi bom, mas poderia ser mais rápido.\"\n",
    "output = chain.invoke({\"frase\": frase_teste})\n",
    "print(\"Sentimento classificado:\", output.strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
